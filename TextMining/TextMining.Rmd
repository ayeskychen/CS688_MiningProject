---
title: "Textmining"
author: "Team3A - Sky Liu"
date: "November 4, 2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="pdf",fig.align  = 'center')
pacman::p_load(
  "ggplot2",
  "knitr",
  "VGAM",
  "rvest",
  "tidyverse",
  "wordcloud",
  "dplyr",
  "tidytext",
  "stringr",
  "tidyr",
  "scales",
  "gridExtra",
  "tm"
)
```

##READ TEXT FILE

```{r readtext,echo=TRUE,warning=FALSE}
#get text from the blog
#read text



text2 <- read.table(file ="2008 Housing crisis.txt",header = FALSE,sep="\n")
text2 <- as.data.frame(text2)
#THE SUBPRIME CRISIS AND HOUSE PRICE APPRECIATION 
text1 <- read.table(file ="The Subprime Crisis and House Price Appreciation.txt",header = FALSE,sep="\n")
```

## GET A TIDY TEXT FORMAT & WORD COUNT
```{r,echo=TRUE,warning=FALSE,results='hide',fig.width=5,fig.height=3}

#remove empty lines
text1 <- text1 %>% filter(V1 != " ")
text1 <- text1 %>% filter(V1 != "\f")
colnames(text1) <- "text"
line <- c(1:length(text1))
text1 <- cbind(line,text1)
text1$text<-as.character(text1$text)
#a token per row
text1 <-text1 %>%unnest_tokens(word,text)
#get rid of any non-characters
text1 <- text1 %>%mutate(word = str_extract(word,"[a-z']+"))
text1 <-na.omit(text1)
#get rid of stop-words
text1<- text1 %>% anti_join(stop_words)
s<-stop_words
#word count
text1 %>%
count(word, sort = TRUE)%>%
  filter(n > 75) %>%
  mutate(word = reorder(word, n))%>%
  ggplot(aes(word, n)) +geom_col() +xlab(NULL) +coord_flip() +ggtitle("Word Count for text 1")




#remove empty lines
text2 <- text2 %>% filter(text2 != " ")
text2 <- text2 %>% filter(text2 != "\f")
colnames(text2) <- "text"
line <- c(1:length(text2))
text2 <- cbind(line,text2)
text2$text<-as.character(text2$text)
#a token per row
text2 <-text2 %>%unnest_tokens(word,text)
#get rid of any non-characters
text2 <- text2 %>%mutate(word = str_extract(word,"[a-z']+"))
text2 <-na.omit(text2)
#get rid of stop-words
text2<- text2 %>% anti_join(stop_words)
s<-stop_words
#word count
text2 %>%
count(word, sort = TRUE)%>%
  filter(n > 50) %>%
  mutate(word = reorder(word, n))%>%
  ggplot(aes(word, n)) +geom_col() +xlab(NULL) +coord_flip() +ggtitle("Word Count for text 2")

```




```{r,echo=FALSE,warning=FALSE,fig.width=10,fig.height=5}
#Comparing the word frequencies of text1 & text2
frequency <- bind_rows(mutate(text2, author = "HC2018"),
                       mutate(text1, author = "SC")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>%
  select(-n) %>%
  spread(author, proportion)

#expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = HC2018, y =SC, color = abs(SC - HC2018))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "red", high = "blue") +
  theme(legend.position="none") + ggtitle("Comparing Word frequencies")
```


From this plot we can see that "mortgages" is frequently used in both papers. The differences is:
"The Subprime Crisis and House Price Appreciation" focuses more on housing price appreciation while "The 2008 Housing Crisis" focuses more on federal/government policy.



##Word Cloud

#Word Cloud of “Subprime Crisis”
```{r,echo=TRUE,warning=FALSE,,fig.width=3,fig.height=3}
text1 %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```
#Word Cloud of “Housing Crisis 2018”
```{r}
text2 %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```

##Sentiment Analysis

#Most common positive and negative words
```{r,echo=TRUE,warning=FALSE,fig.width=10,fig.height=4}
bing_word_counts <- text1 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip() + ggtitle("Most Common Positive and Negative Words in Subprime Crisis")
```

#Sentiment Contribution 

```{r,echo=TRUE,warning=FALSE,,fig.width=10,fig.height=8}

contributions1 <- text1 %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(occurences = n(),
            contribution = sum(score))

contributions1 %>%
  top_n(15, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() + ggtitle("Sentiment Analysis on Subprime Crisis ")

contributions2 <- text2 %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(occurences = n(),
            contribution = sum(score))

contributions2 %>%
  top_n(15, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() + ggtitle("Sentiment Analysis on HousingCrisis2018 ")


```


Because "crisis" appears much more often in "HousingCrisis2018" than in "Subprime Crisis", it takes the most nagetive sentiment contribution in the paper, while in "Subprime Crisis", the effects of "risk" and "unemployment" are more dominant.











##Bigram sentiment Analysis

```{r,echo=TRUE,warning=FALSE,,fig.width=3,fig.height=3}
#taking bigram and filtering out non character and OAs
text11<-read.table(file ="The Subprime Crisis and House Price Appreciation.txt",header = FALSE,sep="\n")
  
text11 <- text11 %>% filter(V1 != " ")
text11 <- text11 %>% filter(V1 != "\f")
colnames(text11) <- "text"
line <- c(1:length(text11))
text11 <- cbind(line,text11)
text11$text<-as.character(text11$text)

text11<- text11%>%unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  mutate(word1 = str_extract(word1,"[a-z']+"))%>%
  na.omit(word1)%>%
  mutate(word2 = str_extract(word2,"[a-z']+"))%>%
  na.omit(word2)

negation_words <- c("not", "no", "never", "without","don't")

#filter out bigrams starts with negation words
negation_words <- text11 %>%
  filter(word1 %in% negation_words) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()


#plot negation words
 negation_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()

```

