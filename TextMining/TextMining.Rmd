---
title: "Textmining"
author: "Sky Liu"
date: "November 4, 2018"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="0.9\\linewidth",dev="pdf",fig.align  = 'center')
pacman::p_load(
  "ggplot2",
  "knitr",
  "VGAM",
  "rvest",
  "tidyverse",
  "wordcloud",
  "dplyr",
  "tidytext",
  "stringr",
  "tidyr",
  "scales",
  "gridExtra",
  "tm"
)
```

##READ TEXT FILE

```{r readtext,echo=TRUE,warning=FALSE}
#get text from the blog
#read text
text1 <- read.table(file ="NAEC_Origins-of-the-Crisis_ENG.txt",header = FALSE,sep="\n")


text2 <- pdf_text("2008 Housing crisis.pdf")
text2 <- as.data.frame(text2)
#THE SUBPRIME CRISIS AND HOUSE PRICE APPRECIATION 
text3 <- read.table(file ="w15334.txt",header = FALSE,sep="\n")
```

## GET A TIDY TEXT FORMAT & WORD COUNT
```{r,echo=TRUE,warning=FALSE,results='hide',fig.width=5,fig.height=3}

#remove empty lines
text1 <- text1 %>% filter(V1 != " ")
text1 <- text1 %>% filter(V1 != "\f")
colnames(text1) <- "text"
line <- c(1:length(text1))
text1 <- cbind(line,text1)
text1$text<-as.character(text1$text)
#a token per row
text1 <-text1 %>%unnest_tokens(word,text)
#get rid of any non-characters
text1 <- text1 %>%mutate(word = str_extract(word,"[a-z']+"))
text1 <-na.omit(text1)
#get rid of stop-words
text1<- text1 %>% anti_join(stop_words)
s<-stop_words
#word count
text1 %>%
count(word, sort = TRUE)%>%
  filter(n > 75) %>%
  mutate(word = reorder(word, n))%>%
  ggplot(aes(word, n)) +geom_col() +xlab(NULL) +coord_flip() +ggtitle("Word Count for text 1")




#remove empty lines
text3 <- text3 %>% filter(text3 != " ")
text3 <- text3 %>% filter(text3 != "\f")
colnames(text3) <- "text"
line <- c(1:length(text3))
text3 <- cbind(line,text3)
text3$text<-as.character(text3$text)
#a token per row
text3 <-text3 %>%unnest_tokens(word,text)
#get rid of any non-characters
text3 <- text3 %>%mutate(word = str_extract(word,"[a-z']+"))
text3 <-na.omit(text3)
#get rid of stop-words
text3<- text3 %>% anti_join(stop_words)
s<-stop_words
#word count
text3 %>%
count(word, sort = TRUE)%>%
  filter(n > 75) %>%
  mutate(word = reorder(word, n))%>%
  ggplot(aes(word, n)) +geom_col() +xlab(NULL) +coord_flip() +ggtitle("Word Count for text 3")

```




```{r,echo=FALSE,warning=FALSE,fig.width=10,fig.height=5}
#Comparing the word frequencies of text1 & text2
# frequency <- bind_rows(mutate(text1, author = "Lisa"),
#                        mutate(text2, author = "Johannes")) %>%
#   count(author, word) %>%
#   group_by(author) %>%
#   mutate(proportion = n / sum(n)) %>% 
#   select(-n) %>% 
#   spread(author, proportion)

# expect a warning about rows with missing values being removed
# ggplot(frequency, aes(x = Johannes, y =Lisa, color = abs(Lisa - Johannes))) +
#   geom_abline(color = "gray40", lty = 2) +
#   geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
#   geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
#   scale_x_log10(labels = percent_format()) +
#   scale_y_log10(labels = percent_format()) +
#   scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
#   theme(legend.position="none") + ggtitle("Comparing Word frequencies")
```



##Sentiment Analysis With Tidy Data

```{r,echo=TRUE,warning=FALSE,,fig.width=10,fig.height=8}

contributions <- text1 %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(occurences = n(),
            contribution = sum(score))

contributions %>%
  top_n(15, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution, fill = contribution > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip()


```






#Comparing the three sentiment dictionaries

```{r,echo=FALSE,warning=FALSE,fig.width=10,fig.height=6}
# #use text1 here as an example
# afinn <- text1 %>% 
#   inner_join(get_sentiments("afinn")) %>% 
#   group_by(index = line %/% 2) %>% 
#   summarise(sentiment = sum(score)) %>% 
#   mutate(method = "AFINN")
# bing_and_nrc <- bind_rows(text1 %>% 
#                             inner_join(get_sentiments("bing")) %>%
#                             mutate(method = "Bing et al."),
#                           text1 %>% 
#                             inner_join(get_sentiments("nrc") %>% 
#                                          filter(sentiment %in% c("positive", 
#                                                                  "negative"))) %>%
#                             mutate(method = "NRC")) %>%
#   count(method, index = line %/% 2, sentiment) %>%
#   spread(sentiment, n, fill = 0) %>%
#   mutate(sentiment = positive - negative)
# bind_rows(afinn, 
#           bing_and_nrc) %>%
#   ggplot(aes(index, sentiment, fill = method)) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~method, ncol = 1, scales = "free_y") + ggtitle("Three Dictionaries Sentiment Analysis on Blog 1")
```



##Most common positive and negative words
```{r,echo=TRUE,warning=FALSE,fig.width=10,fig.height=4}
bing_word_counts <- text1 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip() + ggtitle("Most Common Positive and Negative Words in text1")
```



#Word Cloud
```{r,echo=TRUE,warning=FALSE,,fig.width=3,fig.height=3}
text1 %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```



##Bigram sentiment Analysis

```{r,echo=TRUE,warning=FALSE,,fig.width=3,fig.height=3}
#taking bigram and filtering out non character and OAs
text11<-read.table(file ="NAEC_Origins-of-the-Crisis_ENG.txt",header = FALSE,sep="\n")
  
text11 <- text11 %>% filter(V1 != " ")
text11 <- text11 %>% filter(V1 != "\f")
colnames(text11) <- "text"
line <- c(1:length(text11))
text11 <- cbind(line,text11)
text11$text<-as.character(text11$text)

text11<- text11%>%unnest_tokens(bigram, text, token = "ngrams", n = 2)%>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  mutate(word1 = str_extract(word1,"[a-z']+"))%>%
  na.omit(word1)%>%
  mutate(word2 = str_extract(word2,"[a-z']+"))%>%
  na.omit(word2)

negation_words <- c("not", "no", "never", "without","don't")

#filter out bigrams starts with negation words
negation_words <- text11 %>%
  filter(word1 %in% negation_words) %>%
  inner_join(get_sentiments("afinn"), by = c(word2 = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup()


#plot negation words
 negation_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()

```

